{"cells":[{"cell_type":"markdown","metadata":{"id":"A3eC0o6P0AwF"},"source":["# VQA\n","test accuracy=41%を超えた提出のみ，修了要件として認める"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGPj4VAr2n0X"},"outputs":[],"source":["import os\n","import shutil\n","from concurrent.futures import ThreadPoolExecutor"]},{"cell_type":"markdown","metadata":{"id":"kYJhl4ke2JRB"},"source":["## 環境構築\n","\n","*   コード\n","  * GitHub へ fork\n","  * Google Drive へ clone\n","  * VSCode で編集\n","  * Colab から Google Drive へ cd, コマンドで実行\n","*   学習データ\n","  * 自分の Google Drive に共有元フォルダへのショートカット作成\n","  * ショートカット経由でランタイムにコピー"]},{"cell_type":"markdown","metadata":{"id":"qWWjZp9h2TFv"},"source":["### Google Drive にアクセス"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15207,"status":"ok","timestamp":1720922677819,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"1mo6AE4ow3XI","outputId":"56c2b2f3-49a1-4d84-a29d-8e27347d90c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n","Mounted at /content/drive\n"]}],"source":["# Google Drive をマウント\n","from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1249,"status":"ok","timestamp":1720922679066,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"5JSfEDdO1RSb","outputId":"080469aa-b093-433d-c13e-b78fc0219ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Final\n"," \u001b[0m\u001b[01;34mconfigs\u001b[0m/         \u001b[01;34m'kouho 1'\u001b[0m/        main_original.py   \u001b[01;34mnotebooks\u001b[0m/          \u001b[01;34msrc\u001b[0m/\n"," \u001b[01;34mdata\u001b[0m/            \u001b[01;34m'kouho 2'\u001b[0m/        main.py            README.md           \u001b[01;34msrc_original\u001b[0m/\n"," description.txt  \u001b[01;34m'kouho 3'\u001b[0m/        memo.txt           requirements.txt    submission.npy\n"," Dockerfile        main_kaizen.py   model.pth          sample_train.json   train.py\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Final\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"44YcTrT82Za-"},"source":["### データをランタイムにコピー, 解凍"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183110,"status":"ok","timestamp":1720922862175,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"7FAhEA7p2Y80","outputId":"07f825fa-bc74-4a67-a91e-524ce43cc39a"},"outputs":[{"output_type":"stream","name":"stdout","text":["All files copied successfully.\n"]}],"source":["# コピー元とコピー先のディレクトリを指定\n","src_dir = '/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Final/data/VQA'\n","dst_dir = '/content/data'\n","\n","# コピー先ディレクトリを作成\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","# コピーするファイルのリストを作成\n","files_to_copy = []\n","for root, _, files in os.walk(src_dir):\n","    for file in files:\n","        src_file = os.path.join(root, file)\n","        dst_file = os.path.join(dst_dir, os.path.relpath(src_file, src_dir))\n","        files_to_copy.append((src_file, dst_file))\n","\n","# 並列でファイルをコピーする関数\n","def copy_file(src_dst):\n","    src_file, dst_file = src_dst\n","    dst_file_dir = os.path.dirname(dst_file)\n","    os.makedirs(dst_file_dir, exist_ok=True)\n","    try:\n","        shutil.copy2(src_file, dst_file)\n","    except Exception as e:\n","        print(f\"Error copying {src_file} to {dst_file}: {e}\")\n","\n","# ThreadPoolExecutorを使って並列でコピー\n","with ThreadPoolExecutor(max_workers=24) as executor:\n","    executor.map(copy_file, files_to_copy)\n","\n","# ファイルがすべてコピーされたかを確認\n","copied_files = [os.path.join(root, file) for root, _, files in os.walk(dst_dir) for file in files]\n","missing_files = [src for src, dst in files_to_copy if dst not in copied_files]\n","\n","if missing_files:\n","    print(f\"Missing files: {missing_files}\")\n","else:\n","    print(\"All files copied successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x019rAZp6IXH"},"outputs":[],"source":["# 解凍\n","shutil.unpack_archive('/content/data/train.zip', '/content/data')\n","shutil.unpack_archive('/content/data/valid.zip', '/content/data')"]},{"cell_type":"markdown","metadata":{"id":"TL8wzEch8M28"},"source":["## 実行"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1720922924204,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"bLMRkAFZ9qO6","outputId":"e9b93d4e-50a5-4a97-853c-d1e37da13d59"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Final'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1720922924204,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"1slnLFco88gC","outputId":"c9c7a1dc-f780-4621-8cc4-17b28fa06c13"},"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mconfigs\u001b[0m/         \u001b[01;34m'kouho 1'\u001b[0m/        main_original.py   \u001b[01;34mnotebooks\u001b[0m/          \u001b[01;34msrc\u001b[0m/\n"," \u001b[01;34mdata\u001b[0m/            \u001b[01;34m'kouho 2'\u001b[0m/        main.py            README.md           \u001b[01;34msrc_original\u001b[0m/\n"," description.txt  \u001b[01;34m'kouho 3'\u001b[0m/        memo.txt           requirements.txt    submission.npy\n"," Dockerfile        main_kaizen.py   model.pth          sample_train.json   train.py\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SB5Vjrom8xe6","outputId":"cc2ae666-6354-4f47-e24b-7af49ea23242","executionInfo":{"status":"ok","timestamp":1720962575692,"user_tz":-540,"elapsed":8620326,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["device : cuda\n","- START -\n","\n","【1/200】\n","--- Training ---\n","train time: 505.38 [s]\n","train loss: 8.0832\n","train acc: 0.3622\n","train simple acc: 0.2956\n","--- Test ---\n","['black' 'blue' 'grey' 'no' 'remote control' 'unanswerable' 'white' 'yes']\n","test time: 201.22 [s]\n","\n","【2/200】\n","--- Training ---\n","train time: 505.83 [s]\n","train loss: 5.3585\n","train acc: 0.4653\n","train simple acc: 0.3771\n","--- Test ---\n","['black' 'blue' 'grey' 'no' 'unanswerable' 'white' 'yes']\n","test time: 201.06 [s]\n","\n","【3/200】\n","--- Training ---\n","train time: 504.78 [s]\n","train loss: 4.8313\n","train acc: 0.4854\n","train simple acc: 0.3941\n","--- Test ---\n","['black' 'black white' 'blue' 'candy' 'grey' 'keyboard' 'no' 'pink' 'red'\n"," 'unanswerable' 'white' 'yes']\n","test time: 200.35 [s]\n","\n","【4/200】\n","--- Training ---\n","train time: 506.09 [s]\n","train loss: 4.6236\n","train acc: 0.4948\n","train simple acc: 0.4032\n","--- Test ---\n","['black' 'blue' 'brown' 'green' 'grey' 'no' 'pink' 'purple' 'red'\n"," 'unanswerable' 'white' 'yes']\n","test time: 199.87 [s]\n","\n","【5/200】\n","--- Training ---\n","train time: 503.20 [s]\n","train loss: 4.4719\n","train acc: 0.4994\n","train simple acc: 0.4076\n","--- Test ---\n","['black' 'blue' 'candy bars' 'green' 'grey' 'no' 'pink' 'purple' 'red'\n"," 'unanswerable' 'white' 'yellow' 'yes']\n","test time: 200.08 [s]\n","\n","【6/200】\n","--- Training ---\n","train time: 505.59 [s]\n","train loss: 4.3523\n","train acc: 0.5027\n","train simple acc: 0.4106\n","--- Test ---\n","['black' 'blue' 'brown' 'green' 'grey' 'no' 'pink' 'purple' 'red'\n"," 'unanswerable' 'white' 'yellow' 'yes']\n","test time: 201.40 [s]\n","\n","【7/200】\n","--- Training ---\n","train time: 505.06 [s]\n","train loss: 4.2465\n","train acc: 0.5052\n","train simple acc: 0.4135\n","--- Test ---\n","['black' 'blue' 'brown' 'green' 'grey' 'laptop' 'no' 'pink' 'purple' 'red'\n"," 'unanswerable' 'white' 'yellow' 'yes']\n","test time: 202.30 [s]\n","\n","【8/200】\n","--- Training ---\n","train time: 505.22 [s]\n","train loss: 4.1485\n","train acc: 0.5075\n","train simple acc: 0.4166\n","--- Test ---\n","['black' 'black white' 'blue' 'brown' 'green' 'grey' 'laptop' 'no' 'pink'\n"," 'purple' 'red' 'unanswerable' 'white' 'yes']\n","test time: 201.49 [s]\n","\n","【9/200】\n","--- Training ---\n","train time: 505.80 [s]\n","train loss: 4.0467\n","train acc: 0.5099\n","train simple acc: 0.4193\n","--- Test ---\n","['black' 'black white' 'blue' 'brown' 'green' 'grey' 'keyboard' 'laptop'\n"," 'no' 'pink' 'purple' 'red' 'soup' 'unanswerable' 'white' 'wine' 'yellow'\n"," 'yes']\n","test time: 201.93 [s]\n","\n","【10/200】\n","--- Training ---\n","train time: 504.50 [s]\n","train loss: 3.9529\n","train acc: 0.5117\n","train simple acc: 0.4213\n","--- Test ---\n","['black' 'black white' 'blue' 'brown' 'cell ph1' 'corn' 'green' 'grey'\n"," 'keyboard' 'laptop' 'mustard' 'no' 'pink' 'purple' 'red' 'remote' 'soup'\n"," 'unanswerable' 'white' 'wine' 'yellow' 'yes']\n","test time: 202.54 [s]\n","\n","【11/200】\n","--- Training ---\n","train time: 511.51 [s]\n","train loss: 3.8504\n","train acc: 0.5150\n","train simple acc: 0.4238\n","--- Test ---\n","['black' 'black white' 'blue' 'brown' 'candy bars' 'cell ph1' 'corn'\n"," 'green' 'grey' 'keyboard' 'laptop' 'no' 'off' 'ph1' 'pink' 'purple' 'red'\n"," 'remote' 'remote control' 'soup' 'unanswerable' 'white' 'wine' 'yellow'\n"," 'yes']\n","test time: 206.23 [s]\n","\n","【12/200】\n","--- Training ---\n","train time: 505.81 [s]\n","train loss: 3.7559\n","train acc: 0.5158\n","train simple acc: 0.4267\n","--- Test ---\n","['black' 'black white' 'blue' 'brown' 'cell ph1' 'corn' 'green' 'grey'\n"," 'keyboard' 'laptop' 'no' 'off' 'ph1' 'pink' 'purple' 'red' 'regular'\n"," 'remote' 'soup' 'unanswerable' 'white' 'wine' 'yellow' 'yes']\n","test time: 201.78 [s]\n","\n","【13/200】\n","\n"]}],"source":["!python3 main.py"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1720953948061,"user":{"displayName":"木澤祐人","userId":"14037010150712531752"},"user_tz":-540},"id":"AAWr3BjtASL4","outputId":"036de2f2-c844-4196-d199-e3d1e48961f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["import re\r\n","import random\r\n","import time\r\n","from statistics import mode\r\n","\r\n","from PIL import Image\r\n","import numpy as np\r\n","import pandas as pd\r\n","import torch\r\n","import torch.nn as nn\r\n","import torchvision\r\n","from torchvision import transforms\r\n","\r\n","from transformers import BertModel, BertTokenizer\r\n","import shutil\r\n","import os\r\n","\r\n","\r\n","def set_seed(seed):\r\n","    random.seed(seed)\r\n","    np.random.seed(seed)\r\n","    torch.manual_seed(seed)\r\n","    torch.cuda.manual_seed(seed)\r\n","    torch.cuda.manual_seed_all(seed)\r\n","    torch.backends.cudnn.deterministic = True\r\n","    torch.backends.cudnn.benchmark = False\r\n","\r\n","\r\n","def process_text(text):\r\n","    # lowercase\r\n","    text = text.lower()\r\n","\r\n","    # 数詞を数字に変換\r\n","    num_word_to_digit = {\r\n","        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\r\n","        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\r\n","        'ten': '10'\r\n","    }\r\n","    for word, digit in num_word_to_digit.items():\r\n","        text = text.replace(word, digit)\r\n","\r\n","    # 小数点のピリオドを削除\r\n","    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\r\n","\r\n","    # 冠詞の削除\r\n","    text = re.sub(r'\\b(a|an|the)\\b', '', text)\r\n","\r\n","    # 短縮形のカンマの追加\r\n","    contractions = {\r\n","        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\r\n","        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\r\n","    }\r\n","    for contraction, correct in contractions.items():\r\n","        text = text.replace(contraction, correct)\r\n","\r\n","    # 句読点をスペースに変換\r\n","    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\r\n","\r\n","    # 句読点をスペースに変換\r\n","    text = re.sub(r'\\s+,', ',', text)\r\n","\r\n","    # 連続するスペースを1つに変換\r\n","    text = re.sub(r'\\s+', ' ', text).strip()\r\n","\r\n","    return text\r\n","\r\n","\r\n","# 1. データローダーの作成\r\n","class VQADataset(torch.utils.data.Dataset):\r\n","    def __init__(self, df_path, image_dir, transform=None, answer=True, question_one_hot=True):\r\n","        self.transform = transform  # 画像の前処理\r\n","        self.image_dir = image_dir  # 画像ファイルのディレクトリ\r\n","        self.df = pd.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\r\n","        self.answer = answer\r\n","        self.question_one_hot = question_one_hot # 追加\r\n","\r\n","        # question / answerの辞書を作成\r\n","        self.question2idx = {}\r\n","        self.answer2idx = {}\r\n","        self.idx2question = {}\r\n","        self.idx2answer = {}\r\n","\r\n","        # 質問文に含まれる単語を辞書に追加\r\n","        for question in self.df[\"question\"]:\r\n","            question = process_text(question)\r\n","            words = question.split(\" \")\r\n","            for word in words:\r\n","                if word not in self.question2idx:\r\n","                    self.question2idx[word] = len(self.question2idx)\r\n","        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\r\n","\r\n","        if self.answer:\r\n","            # 回答に含まれる単語を辞書に追加\r\n","            for answers in self.df[\"answers\"]:\r\n","                for answer in answers:\r\n","                    word = answer[\"answer\"]\r\n","                    word = process_text(word)\r\n","                    if word not in self.answer2idx:\r\n","                        self.answer2idx[word] = len(self.answer2idx)\r\n","            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\r\n","\r\n","    def update_dict(self, dataset):\r\n","        \"\"\"\r\n","        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        dataset : Dataset\r\n","            訓練データのDataset\r\n","        \"\"\"\r\n","        self.question2idx = dataset.question2idx\r\n","        self.answer2idx = dataset.answer2idx\r\n","        self.idx2question = dataset.idx2question\r\n","        self.idx2answer = dataset.idx2answer\r\n","\r\n","    def __getitem__(self, idx):\r\n","        \"\"\"\r\n","        対応するidxのデータ（画像，質問，回答）を取得．\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        idx : int\r\n","            取得するデータのインデックス\r\n","\r\n","        Returns\r\n","        -------\r\n","        image : torch.Tensor  (C, H, W)\r\n","            画像データ\r\n","        question : torch.Tensor  (vocab_size)\r\n","            質問文をone-hot表現に変換したもの\r\n","        answers : torch.Tensor  (n_answer)\r\n","            10人の回答者の回答のid\r\n","        mode_answer_idx : torch.Tensor  (1)\r\n","            10人の回答者の回答の中で最頻値の回答のid\r\n","        \"\"\"\r\n","        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\r\n","        image = self.transform(image)\r\n","\r\n","        if self.question_one_hot:\r\n","            question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\r\n","            question_words = self.df[\"question\"][idx].split(\" \")\r\n","            for word in question_words:\r\n","                try:\r\n","                    question[self.question2idx[word]] = 1  # one-hot表現に変換\r\n","                except KeyError:\r\n","                    question[-1] = 1  # 未知語\r\n","            question = torch.Tensor(question)\r\n","\r\n","        else: \r\n","            # 質問文をテキストのまま返す\r\n","            question = self.df[\"question\"][idx]\r\n","\r\n","        if self.answer:\r\n","            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\r\n","            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\r\n","\r\n","            return image, question, torch.Tensor(answers), int(mode_answer_idx)\r\n","\r\n","        else:\r\n","            return image, question\r\n","\r\n","    def __len__(self):\r\n","        return len(self.df)\r\n","    \r\n","class VQADataset_class_mapping(torch.utils.data.Dataset):\r\n","    def __init__(self, df_path, image_dir, transform=None, answer=True, question_one_hot=True):\r\n","        self.transform = transform  # 画像の前処理\r\n","        self.image_dir = image_dir  # 画像ファイルのディレクトリ\r\n","        self.df = pd.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\r\n","        self.answer = answer\r\n","        self.question_one_hot = question_one_hot # 追加\r\n","\r\n","        # question / answerの辞書を作成\r\n","        self.question2idx = {}\r\n","        self.answer2idx = {}\r\n","        self.idx2question = {}\r\n","        self.idx2answer = {}\r\n","\r\n","        # 質問文に含まれる単語を辞書に追加\r\n","        for question in self.df[\"question\"]:\r\n","            question = process_text(question)\r\n","            words = question.split(\" \")\r\n","            for word in words:\r\n","                if word not in self.question2idx:\r\n","                    self.question2idx[word] = len(self.question2idx)\r\n","        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\r\n","\r\n","        # 回答に含まれる単語(文 ?)を辞書に追加\r\n","        if self.answer:\r\n","            answer_copus = pd.read_csv(\"data/data_annotations_class_mapping.csv\")\r\n","            self.answer2idx = dict(zip(answer_copus[\"answer\"], answer_copus[\"class_id\"]))\r\n","            self.idx2answer = {v: k for k, v in self.answer2idx.items()}\r\n","\r\n","            for answers in self.df[\"answers\"]:\r\n","                for answer in answers:\r\n","                    word = answer[\"answer\"]\r\n","                    word = process_text(word)\r\n","                    if word not in self.answer2idx:\r\n","                        self.answer2idx[word] = len(self.answer2idx)\r\n","            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\r\n","\r\n","    def update_dict(self, dataset):\r\n","        \"\"\"\r\n","        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        dataset : Dataset\r\n","            訓練データのDataset\r\n","        \"\"\"\r\n","        self.question2idx = dataset.question2idx\r\n","        self.answer2idx = dataset.answer2idx\r\n","        self.idx2question = dataset.idx2question\r\n","        self.idx2answer = dataset.idx2answer\r\n","\r\n","    def __getitem__(self, idx):\r\n","        \"\"\"\r\n","        対応するidxのデータ（画像，質問，回答）を取得．\r\n","\r\n","        Parameters\r\n","        ----------\r\n","        idx : int\r\n","            取得するデータのインデックス\r\n","\r\n","        Returns\r\n","        -------\r\n","        image : torch.Tensor  (C, H, W)\r\n","            画像データ\r\n","        question : torch.Tensor  (vocab_size)\r\n","            質問文をone-hot表現に変換したもの\r\n","        answers : torch.Tensor  (n_answer)\r\n","            10人の回答者の回答のid\r\n","        mode_answer_idx : torch.Tensor  (1)\r\n","            10人の回答者の回答の中で最頻値の回答のid\r\n","        \"\"\"\r\n","        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\r\n","        image = self.transform(image)\r\n","\r\n","        if self.question_one_hot:\r\n","            question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\r\n","            question_words = self.df[\"question\"][idx].split(\" \")\r\n","            for word in question_words:\r\n","                try:\r\n","                    question[self.question2idx[word]] = 1  # one-hot表現に変換\r\n","                except KeyError:\r\n","                    question[-1] = 1  # 未知語\r\n","            question = torch.Tensor(question)\r\n","\r\n","        else: \r\n","            # 質問文をテキストのまま返す\r\n","            question = self.df[\"question\"][idx]\r\n","\r\n","        if self.answer:\r\n","            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\r\n","            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\r\n","\r\n","            return image, question, torch.Tensor(answers), int(mode_answer_idx)\r\n","\r\n","        else:\r\n","            return image, question\r\n","\r\n","    def __len__(self):\r\n","        return len(self.df)\r\n","\r\n","\r\n","\r\n","\r\n","# 2. 評価指標の実装\r\n","# 簡単にするならBCEを利用する\r\n","def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\r\n","    total_acc = 0.\r\n","\r\n","    for pred, answers in zip(batch_pred, batch_answers):\r\n","        acc = 0.\r\n","        for i in range(len(answers)):\r\n","            num_match = 0\r\n","            for j in range(len(answers)):\r\n","                if i == j:\r\n","                    continue\r\n","                if pred == answers[j]:\r\n","                    num_match += 1\r\n","            acc += min(num_match / 3, 1)\r\n","        total_acc += acc / 10\r\n","\r\n","    return total_acc / len(batch_pred)\r\n","\r\n","\r\n","# 3. モデルのの実装\r\n","# ResNetを利用できるようにしておく\r\n","class BasicBlock(nn.Module):\r\n","    expansion = 1\r\n","\r\n","    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\r\n","        super().__init__()\r\n","\r\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\r\n","        self.bn1 = nn.BatchNorm2d(out_channels)\r\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\r\n","        self.bn2 = nn.BatchNorm2d(out_channels)\r\n","        self.relu = nn.ReLU(inplace=True)\r\n","\r\n","        self.shortcut = nn.Sequential()\r\n","        if stride != 1 or in_channels != out_channels:\r\n","            self.shortcut = nn.Sequential(\r\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\r\n","                nn.BatchNorm2d(out_channels)\r\n","            )\r\n","\r\n","    def forward(self, x):\r\n","        residual = x\r\n","        out = self.relu(self.bn1(self.conv1(x)))\r\n","        out = self.bn2(self.conv2(out))\r\n","\r\n","        out += self.shortcut(residual)\r\n","        out = self.relu(out)\r\n","\r\n","        return out\r\n","\r\n","\r\n","class BottleneckBlock(nn.Module):\r\n","    expansion = 4\r\n","\r\n","    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\r\n","        super().__init__()\r\n","\r\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\r\n","        self.bn1 = nn.BatchNorm2d(out_channels)\r\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\r\n","        self.bn2 = nn.BatchNorm2d(out_channels)\r\n","        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\r\n","        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\r\n","        self.relu = nn.ReLU(inplace=True)\r\n","\r\n","        self.shortcut = nn.Sequential()\r\n","        if stride != 1 or in_channels != out_channels * self.expansion:\r\n","            self.shortcut = nn.Sequential(\r\n","                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\r\n","                nn.BatchNorm2d(out_channels * self.expansion)\r\n","            )\r\n","\r\n","    def forward(self, x):\r\n","        residual = x\r\n","        out = self.relu(self.bn1(self.conv1(x)))\r\n","        out = self.relu(self.bn2(self.conv2(out)))\r\n","        out = self.bn3(self.conv3(out))\r\n","\r\n","        out += self.shortcut(residual)\r\n","        out = self.relu(out)\r\n","\r\n","        return out\r\n","\r\n","\r\n","class ResNet(nn.Module):\r\n","    def __init__(self, block, layers):\r\n","        super().__init__()\r\n","        self.in_channels = 64\r\n","\r\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\r\n","        self.bn1 = nn.BatchNorm2d(64)\r\n","        self.relu = nn.ReLU(inplace=True)\r\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n","\r\n","        self.layer1 = self._make_layer(block, layers[0], 64)\r\n","        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\r\n","        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\r\n","        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\r\n","\r\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n","        self.fc = nn.Linear(512 * block.expansion, 512)\r\n","\r\n","    def _make_layer(self, block, blocks, out_channels, stride=1):\r\n","        layers = []\r\n","        layers.append(block(self.in_channels, out_channels, stride))\r\n","        self.in_channels = out_channels * block.expansion\r\n","        for _ in range(1, blocks):\r\n","            layers.append(block(self.in_channels, out_channels))\r\n","\r\n","        return nn.Sequential(*layers)\r\n","\r\n","    def forward(self, x):\r\n","        x = self.relu(self.bn1(self.conv1(x)))\r\n","        x = self.maxpool(x)\r\n","\r\n","        x = self.layer1(x)\r\n","        x = self.layer2(x)\r\n","        x = self.layer3(x)\r\n","        x = self.layer4(x)\r\n","\r\n","        x = self.avgpool(x)\r\n","        x = x.view(x.size(0), -1)\r\n","        x = self.fc(x)\r\n","\r\n","        return x\r\n","\r\n","\r\n","def ResNet18():\r\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\r\n","\r\n","\r\n","def ResNet50():\r\n","    return ResNet(BottleneckBlock, [3, 4, 6, 3])\r\n","\r\n","\r\n","class VQAModel(nn.Module):\r\n","    def __init__(self, vocab_size: int, n_answer: int):\r\n","        super().__init__()\r\n","        self.resnet = ResNet50() # 18\r\n","        self.text_encoder = nn.Linear(vocab_size, 512)\r\n","\r\n","        self.fc = nn.Sequential(\r\n","            nn.Linear(1024, 512),\r\n","            nn.ReLU(inplace=True),\r\n","            nn.Linear(512, n_answer)\r\n","        )\r\n","\r\n","    def forward(self, image, question):\r\n","        question.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") # BERTへの対応によりdataset.getitem時点ではまだcpuなので追加\r\n","\r\n","        image_feature = self.resnet(image)  # 画像の特徴量\r\n","        question_feature = self.text_encoder(question)  # テキストの特徴量\r\n","\r\n","        x = torch.cat([image_feature, question_feature], dim=1)\r\n","        x = self.fc(x)\r\n","\r\n","        return x\r\n","    \r\n","# BERTでテキストの特徴量抽出\r\n","class VQAModel_BERT(nn.Module):\r\n","    def __init__(self, n_answer: int):\r\n","        super().__init__()\r\n","\r\n","        # BERT\r\n","        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n","        self.bert_model = BertModel.from_pretrained(\r\n","            \"bert-base-uncased\", torch_dtype=torch.float32, attn_implementation=\"sdpa\"\r\n","        )\r\n","        for param in self.bert_model.parameters():\r\n","            param.requires_grad = False\r\n","\r\n","        self.resnet = ResNet50() # 18\r\n","        #self.text_encoder = nn.Linear(vocab_size, 512)\r\n","\r\n","        self.fc = nn.Sequential(\r\n","            #nn.Linear(1024, 512),\r\n","            nn.Linear(512 + 768, 1024),\r\n","            nn.BatchNorm1d(num_features=1024),\r\n","            nn.ReLU(inplace=True),\r\n","            nn.Linear(1024, 1024),\r\n","            nn.BatchNorm1d(num_features=1024),\r\n","            nn.ReLU(inplace=True),\r\n","            nn.Linear(1024, n_answer)\r\n","        )\r\n","\r\n","    def forward(self, image, question):\r\n","        image_feature = self.resnet(image)  # 画像の特徴量 (N, 512)\r\n","        \r\n","        #question_feature = self.text_encoder(question) \r\n","        with torch.no_grad():\r\n","            # question 前処理\r\n","            # question: tuple\r\n","            question = tuple(map(process_text, list(question)))\r\n","            question = self.bert_tokenizer(\r\n","                question,\r\n","                truncation=True,\r\n","                padding=True,\r\n","                return_tensors=\"pt\",\r\n","            ).to(image.device)\r\n","            question_feature = self.bert_model(**question).last_hidden_state[\r\n","                :, 0, :\r\n","            ]  # テキストの特徴量 (N, 768)\r\n","\r\n","        x = torch.cat([image_feature, question_feature], dim=1)\r\n","        x = self.fc(x)\r\n","\r\n","        return x\r\n","\r\n","\r\n","\r\n","\r\n","# 4. 学習の実装\r\n","def train(model, dataloader, optimizer, criterion, device):\r\n","    model.train()\r\n","\r\n","    total_loss = 0\r\n","    total_acc = 0\r\n","    simple_acc = 0\r\n","\r\n","    start = time.time()\r\n","    for image, question, answers, mode_answer in dataloader:\r\n","        image, question, answer, mode_answer = \\\r\n","            image.to(device), question, answers.to(device), mode_answer.to(device)\r\n","\r\n","        pred = model(image, question)\r\n","        loss = criterion(pred, mode_answer.squeeze())  # batch size 32にすると、19873(mod 32)=1 より、mode_answerの形状が[1,]となり、squeeze()で消滅？\r\n","\r\n","        optimizer.zero_grad()\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","\r\n","        total_loss += loss.item()\r\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\r\n","        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\r\n","\r\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\r\n","\r\n","\r\n","def eval(model, dataloader, optimizer, criterion, device):\r\n","    model.eval()\r\n","\r\n","    total_loss = 0\r\n","    total_acc = 0\r\n","    simple_acc = 0\r\n","\r\n","    start = time.time()\r\n","    for image, question, answers, mode_answer in dataloader:\r\n","        image, question, answer, mode_answer = \\\r\n","            image.to(device), question, answers.to(device), mode_answer.to(device)\r\n","\r\n","        pred = model(image, question)\r\n","        loss = criterion(pred, mode_answer.squeeze())\r\n","\r\n","        total_loss += loss.item()\r\n","        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\r\n","        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\r\n","\r\n","    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\r\n","\r\n","\r\n","def main():\r\n","    # deviceの設定\r\n","    set_seed(42)\r\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n","    print(f\"device : {device}\")\r\n","\r\n","    if os.path.exists('archive'):\r\n","        shutil.rmtree(('archive'))\r\n","    os.makedirs('archive')\r\n","\r\n","    # dataloader / model\r\n","    transform = transforms.Compose([\r\n","        transforms.Resize((224, 224)),\r\n","        transforms.RandomHorizontalFlip(p=0.5),\r\n","        transforms.RandomVerticalFlip(p=0.5),\r\n","        transforms.ToTensor()\r\n","    ])\r\n","    \r\n","    # --- original\r\n","    #train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\r\n","    #test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\r\n","    # --- class mapping なし + Bert\r\n","    train_dataset = VQADataset(df_path=\"/content/data/train.json\", image_dir=\"/content/data/train\", transform=transform, question_one_hot=False)\r\n","    test_dataset = VQADataset(df_path=\"/content/data/valid.json\", image_dir=\"/content/data/valid\", transform=transform, answer=False, question_one_hot=False)\r\n","    # --- class mapping + Bert\r\n","    #train_dataset = VQADataset_class_mapping(df_path=\"/content/data/train.json\", image_dir=\"/content/data/train\", transform=transform, question_one_hot=False)\r\n","    #test_dataset = VQADataset_class_mapping(df_path=\"/content/data/valid.json\", image_dir=\"/content/data/valid\", transform=transform, answer=False, question_one_hot=False)\r\n","    \r\n","    test_dataset.update_dict(train_dataset)\r\n","\r\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) # batch_size 32はだめ\r\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\r\n","\r\n","    #model = VQAModel(vocab_size=len(train_dataset.question2idx)+1, n_answer=len(train_dataset.answer2idx)).to(device)\r\n","    model = VQAModel_BERT(n_answer=len(train_dataset.answer2idx)).to(device)\r\n","    #print(\"------\")\r\n","    #print(f\"n_answer = {len(train_dataset.answer2idx)}\")\r\n","    #print(train_dataset.answer2idx)\r\n","    #print(f\"n_answer = {len(train_dataset.idx2answer)}\")\r\n","    #print(train_dataset.idx2answer)\r\n","    #print(f\"n_question = {len(train_dataset.question2idx)}\")\r\n","    #print(train_dataset.question2idx)\r\n","    #print(f\"n_question = {len(train_dataset.idx2question)}\")\r\n","    #print(train_dataset.idx2question)\r\n","    #print(\"---\")\r\n","    #print(f\"n_answer = {len(test_dataset.answer2idx)}\")\r\n","    #print(test_dataset.answer2idx)\r\n","    #print(f\"n_answer = {len(test_dataset.idx2answer)}\")\r\n","    #print(test_dataset.idx2answer)\r\n","    #print(f\"n_question = {len(test_dataset.question2idx)}\")\r\n","    #print(test_dataset.question2idx)\r\n","    #print(f\"n_question = {len(test_dataset.idx2question)}\")\r\n","    #print(test_dataset.idx2question)\r\n","    #print(\"------\")\r\n","\r\n","    # optimizer / criterion\r\n","    num_epoch = 200\r\n","    criterion = nn.CrossEntropyLoss()\r\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.00003, weight_decay=1e-5) # 0.0001\r\n","\r\n","    # train model\r\n","    print(\"- START -\")\r\n","    for epoch in range(num_epoch):\r\n","        print(\"\")\r\n","        print(f\"【{epoch + 1}/{num_epoch}】\")\r\n","        train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\r\n","        print(\"--- Training ---\\n\"\r\n","              f\"train time: {train_time:.2f} [s]\\n\"\r\n","              f\"train loss: {train_loss:.4f}\\n\"\r\n","              f\"train acc: {train_acc:.4f}\\n\"\r\n","              f\"train simple acc: {train_simple_acc:.4f}\")\r\n","        #torch.save(model.state_dict(), f\"archive/model_{epoch + 1}.pth\")\r\n","\r\n","\r\n","        print(\"--- Test ---\")\r\n","        model.eval()\r\n","        submission = []\r\n","        start = time.time()\r\n","        for image, question in test_loader:\r\n","            image, question = image.to(device), question\r\n","            pred = model(image, question)\r\n","            pred = pred.argmax(1).cpu().item()\r\n","            submission.append(pred)\r\n","\r\n","        submission = [train_dataset.idx2answer[id] for id in submission]\r\n","        submission = np.array(submission)\r\n","        print(np.unique(submission))\r\n","        #torch.save(model.state_dict(), f\"archive/model_{epoch + 1}.pth\")\r\n","        np.save(f\"archive/submission_{epoch + 1}.npy\", submission)\r\n","        print(f\"test time: {time.time() - start:.2f} [s]\")\r\n","\r\n","\r\n","\r\n","\r\n","    # 提出用ファイルの作成\r\n","    print(\"--- Evaluation Start ---\")\r\n","    model.eval()\r\n","    submission = []\r\n","    for image, question in test_loader:\r\n","        image, question = image.to(device), question\r\n","        pred = model(image, question)\r\n","        pred = pred.argmax(1).cpu().item()\r\n","        submission.append(pred)\r\n","\r\n","    submission = [train_dataset.idx2answer[id] for id in submission]\r\n","    submission = np.array(submission)\r\n","    torch.save(model.state_dict(), \"model.pth\")\r\n","    np.save(\"submission.npy\", submission)\r\n","\r\n","if __name__ == \"__main__\":\r\n","    main()\r\n"]}],"source":["# 同期状況を確認\n","%cat main.py"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPjk9X+E7KnRZuhJPVFiYW7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}